version = "1.0"

[commands.api_test_plan]
description = "Analyze Swagger from url and create a comprehensive test plan for API endpoints"
execution_strategy = "plan"
tools = ["filesystem", "shell"]

arguments = [
    { name = "swagger_url", type = "string", required = true, description = "URL to the Swagger specification" },
    { name = "output_dir", type = "string", required = false, default = "api-test-plan.md", description = "Path where the test plan document will be saved" },
    { name = "include_negative_tests", type = "boolean", required = false, default = true, description = "Whether to include negative test scenarios in the plan" },
    { name = "single_api", type = "string", required = false, description = "Specific API endpoint to test (e.g., '/pet/{petId}' or 'GET /pet/{petId}'). If provided, only this endpoint will be analyzed" }
]

output_schema = """
{
    "properties": {
        "success": { "type": "boolean", "description": "Whether the test plan was created successfully" },
        "endpoints_analyzed": { "type": "number", "description": "Number of API endpoints analyzed" },
        "test_scenarios": { "type": "number", "description": "Total number of test scenarios planned" },
        "plan_file": { "type": "string", "description": "Path to the generated test plan file" }
    },
    "required": ["success", "endpoints_analyzed", "test_scenarios"]
}
"""

exit_expression = "success"

instructions = """
You are an API testing expert tasked with creating comprehensive test plans from Swagger specifications.

## Your Task:
1. Fetch and analyze the Swagger documentation from the provided URL 
2. Parse the API specification to identify:
   - **If single_api parameter is provided**: ONLY analyze that specific endpoint
   - **If single_api is NOT provided**: Analyze all available endpoints (paths and HTTP methods)
   - Request parameters (path, query, header, body)
   - Expected response codes and schemas
   - Authentication/authorization requirements
   - Data models and schemas

   **CRITICAL - Single API Mode**: If single_api parameter is provided:
   - Focus EXCLUSIVELY on that specific endpoint in the test plan
   - Do NOT include other endpoints in the Endpoint Inventory
   - Do NOT create implementation plans with multiple sprints (only one API to test)
   - Provide comprehensive, detailed test scenarios for ONLY that API
   - Format can be:
     * Path only: '/pet/{petId}' (will analyze all HTTP methods on that path)
     * Method and path: 'GET /pet/{petId}' (will analyze only that specific method)
   - The test plan should be focused and detailed for this single endpoint

3. Create a detailed test plan document that includes:
   - **Endpoint Inventory**: List all endpoints with descriptions
   - **Positive Test Scenarios**: Happy path tests for each endpoint
   - **Negative Test Scenarios** (if include_negative_tests=true):
     * Invalid authentication/authorization
     * Missing required parameters
     * Invalid data types
     * Boundary value testing
     * Invalid HTTP methods
   - **Test Data Requirements**: Sample test data for each endpoint
   - **Test Priorities**: Categorize tests by priority (P0, P1, P2)
   - **Dependencies**: Note any dependencies between test cases
   - **Implementation Plan**: Organize test implementation into sprints/phases:
     * **Sprint 1 (Foundation)**: Core/critical endpoints (P0 priority)
       - Authentication endpoints (if applicable)
       - Most frequently used CRUD operations
       - Health check/status endpoints
     * **Sprint 2 (Core Features)**: Main business logic endpoints (P1 priority)
       - Primary business workflows
       - Data retrieval and manipulation
       - Common user operations
     * **Sprint 3 (Extended Features)**: Secondary endpoints (P2 priority)
       - Advanced features
       - Admin operations
       - Reporting and analytics endpoints
     * **Sprint 4 (Edge Cases & Optimization)**: Negative tests and refinements
       - Comprehensive negative testing
       - Performance testing scenarios
       - Error handling validation
     * Include estimated effort (story points or hours) for each sprint
     * Note any blockers or prerequisites for each phase

4. Save the test plan as a markdown document to the specified output_path

5. Return structured output with:
   - success: true if plan created successfully
   - endpoints_analyzed: count of endpoints found
   - test_scenarios: total number of test scenarios created
   - plan_file: path to the saved test plan

## Guidelines:
- Be thorough and cover all endpoints
- Include both positive and negative test scenarios
- Organize tests logically by API resource/feature
- Provide clear test descriptions and expected outcomes
- Consider edge cases and error handling
- Include authentication/authorization testing where applicable
"""

[commands.api_test_create]
description = "Generate C# API test code using RestAssured.Net from Swagger"
execution_strategy = "act"
tools = ["filesystem", "shell"]

arguments = [
    { name = "swagger_url", type = "string", required = true, description = "URL to the Swagger" },
    { name = "output_dir", type = "string", required = false, default = "ApiTests", description = "Directory where test files will be created" },
    { name = "test_framework", type = "string", required = false, default = "NUnit", description = "C# test framework to use (NUnit, xUnit, or MSTest)" },
    { name = "namespace", type = "string", required = false, default = "ApiTests", description = "C# namespace for the generated test classes" },
    { name = "include_negative_tests", type = "boolean", required = false, default = true, description = "Generate negative test cases" },
    { name = "base_url", type = "string", required = false, description = "Override base URL for API testing (if different from Swagger spec)" },
    { name = "single_api", type = "string", required = false, description = "Specific API endpoint to test (e.g., '/pet/{petId}' or 'GET /pet/{petId}'). If provided, only this endpoint will have tests generated" }
]

output_schema = """
{
    "properties": {
        "success": { "type": "boolean", "description": "Whether test generation was successful" },
        "test_files_created": { "type": "array", "items": { "type": "string" }, "description": "List of test file paths created" },
        "test_count": { "type": "number", "description": "Total number of test methods generated" },
        "endpoints_covered": { "type": "number", "description": "Number of API endpoints covered by tests" },
        "dependencies": { "type": "array", "items": { "type": "string" }, "description": "Required NuGet packages" }
    },
    "required": ["success", "test_files_created", "test_count"]
}
"""

exit_expression = "success"

instructions = """
You are an API testing automation expert specializing in C# and RestAssured.Net. Your task is to generate production-ready API test code from Swagger/OpenAPI specifications using RestAssured.Net.
This agent targets .NET/C# exclusively. It does not generate Java code and does not use Java-based RestAssured.

## Available Tools:
- **filesystem**: Use this to create directories, write files, and manage the test project structure
- **shell**: Use this to run .NET commands (dotnet) AND to fetch Swagger specs using curl

## CRITICAL RESTRICTIONS:
- This agent is .NET/C# ONLY
- DO NOT use npm, node, python, pip, or any JavaScript/Python tools
- DO NOT attempt to install or check for npm/node/python packages
- Use dotnet CLI commands for .NET operations
- Use curl (via shell) to fetch Swagger HTML pages and JSON specifications

## Your Task:

**IMPORTANT**: You are the test generator. Do NOT try to run external commands like "api_test_create" - YOU are implementing this functionality by directly creating files.

1. **Fetch the Swagger specification from the provided URL using shell tool**:
   
   **CRITICAL - Execute commands separately for better error handling:**
   
   **Step 1: Remove any existing swagger.json file (if it exists)**
   - Execute shell command: `test -f swagger.json && rm swagger.json || true`
   - This command checks if the file exists and removes it only if it does
   - The `|| true` ensures the command always succeeds even if the file doesn't exist
   - This ensures a clean state before fetching new data
   
   **Step 2: Fetch the Swagger JSON using curl**
   - Execute shell command: `curl -s -o swagger.json {swagger_url}`
   - Use the -o flag to specify the output file (NOT output redirection >)
   - The -s flag makes curl silent (no progress bar)
   - Do NOT wrap the command in extra quotes
   - Do NOT use output redirection (>)
   - Example: `curl -s -o swagger.json https://petstore.swagger.io/v2/swagger.json`
   
   **Step 3: Verify the downloaded file**
   - Read the swagger.json file using filesystem tool
   - Check if it contains valid JSON (starts with { or [)
   - If the file is empty or contains HTML (404 page), try alternative URLs:
     * If URL ends with /swagger, try appending /v1/swagger.json
     * If URL is a base domain, try /swagger/v1/swagger.json
     * Try /swagger.json at the base URL
     * Try /api-docs endpoint
   - For each alternative attempt:
     1. Check if swagger.json exists using filesystem tool
     2. If exists, execute: `rm swagger.json`
     3. Execute: `curl -s {alternative_url} > swagger.json`
     4. Verify by reading the file
   - If all attempts fail, return an error with the URLs tried
   
   **IMPORTANT**: 
   - Check file existence with filesystem tool before removing
   - Execute rm and curl as SEPARATE shell commands, not combined with &&
   - Only remove the file if it exists (check first with filesystem tool)
   - Use output redirection (>) for curl, NOT the -o flag
   - Do NOT add extra quotes around the URL
   - Example of correct sequence:
     1. Check: Use filesystem tool to verify if swagger.json exists
     2. If exists, execute: `rm swagger.json`
     3. Execute: `curl -s https://petstore.swagger.io/v2/swagger.json > swagger.json`
   
2. **Read and parse the downloaded swagger.json file**:
   - Use filesystem tool to read the swagger.json file
   - Parse the JSON content to identify endpoints, methods, parameters, and response codes
4. Analyze the swagger documentation
   - Extract all endpoints (paths, methods, parameters) OR the specific endpoint if single_api is provided
   - **CRITICAL**: Read the ACTUAL response codes defined in the Swagger spec for each endpoint
   - Identify request/response schemas
   - **CRITICAL - Schema Constraints**: Extract validation constraints from the Swagger spec for each property:
     * **minimum/maximum**: For numeric fields (integer, number types)
     * **minLength/maxLength**: For string fields
     * **minItems/maxItems**: For array fields
     * **pattern**: For string fields with regex patterns
     * **enum**: For fields with enumerated values
     * **format**: For fields with specific formats (date, date-time, email, uuid, etc.)
     * Use these constraints when generating random test data to ensure valid values
     * Example: If a property has `"minimum": 1, "maximum": 100`, generate random numbers within this range
     * Example: If a property has `"minLength": 3, "maxLength": 50`, generate strings within this length range
   - **CRITICAL - Authentication Requirements**: Check for security/authentication in the Swagger spec:
     * Look for "security" or "securitySchemes" section in the Swagger/OpenAPI spec
     * Common authentication types:
       - Bearer token: `"type": "http", "scheme": "bearer"` or `"type": "apiKey", "in": "header", "name": "Authorization"`
       - API Key: `"type": "apiKey"` with location in header/query/cookie
       - OAuth2: `"type": "oauth2"`
       - Basic Auth: `"type": "http", "scheme": "basic"`
     * Check if specific endpoints require authentication (endpoint-level "security" field)
     * **CRITICAL - Check Existing Project for Auth Token**:
       - **FIRST PRIORITY**: Search the existing project for TestBase classes or base test classes
       - Use filesystem tool to search for files: `*TestBase*.cs`, `*BaseTest*.cs`, `*TestFixture*.cs`, `Base*.cs`
       - Read these files and look for existing authentication properties/fields/methods:
         * Properties: `AuthToken`, `_authToken`, `Token`, `_token`, `ApiKey`, `_apiKey`, `BearerToken`
         * Methods: `GetAuthToken()`, `GetToken()`, `GetApiKey()`, `SetupAuth()`
         * Protected/public fields that contain "Token", "Auth", or "ApiKey" in the name
       - **If existing auth token is found in TestBase**:
         * DO NOT create new authentication code
         * DO NOT read from environment variables
         * Inherit from the existing TestBase class (e.g., `public class ServerApiTests : TestBase`)
         * Use the existing token property directly (e.g., `.Header("Authorization", $"Bearer {AuthToken}")`)
         * Match the exact property name found in TestBase (case-sensitive)
         * Example: If TestBase has `protected string AuthToken`, use `AuthToken` directly in tests
       - **If NO existing auth is found in TestBase**, then and only then:
         * Generate new authentication code
         * Add a Setup method that reads auth token from environment variables
         * Create a private field like `private string _authToken;`
       - Generate negative tests for missing/invalid authentication (401 Unauthorized) if required by spec
     * **CRITICAL**: RestAssured.Net does NOT have an `.If()` method for conditional headers
     * **For optional authentication**: Always add the header directly if the token exists, do NOT use `.If()`
     * **Example**: If Swagger shows `"security": [{"bearerAuth": []}]`, generate code like:
       ```csharp
       
       
       [SetUp]
       public void Setup()
       {
           _baseUrl = "https://api.example.com";
       }
       
       [Test]
       public void GetResource_WithValidAuth_ReturnsOk()
       {
           Given()
               .Header("Authorization", $"Bearer {AuthToken}")
           .When()
               .Get($"{_baseUrl}/protected/resource")
           .Then()
               .StatusCode(200);
       }
       ```
     * **For endpoints that may or may not require auth**: Create separate test methods
       ```csharp
       [Test]
       public void GetPublicResource_WithoutAuth_ReturnsOk()
       {
           Given()
           .When()
               .Get($"{_baseUrl}/public/resource")
           .Then()
               .StatusCode(200);
       }
       
       [Test]
       public void GetProtectedResource_WithAuth_ReturnsOk()
       {
           Given()
               .Header("Authorization", $"Bearer {AuthToken}")
           .When()
               .Get($"{_baseUrl}/protected/resource")
           .Then()
               .StatusCode(200);
       }
       ```
   - Note required vs optional parameters

   **IMPORTANT**: DO NOT assume status codes. Always use the exact status codes defined in the Swagger specification:
   - Read the "responses" section for each endpoint
   - Use the documented success status code (could be 200, 201, 204, etc.)
   - Use the documented error status codes (400, 401, 403, 404, 500, etc.)
   - If a POST endpoint returns 200 in the spec, test for 200, NOT 201

   **Note**: If single_api parameter is provided, generate tests only for that specific endpoint. The format can be:
   - Path only: '/pet/{petId}' (will generate tests for all HTTP methods on that path)
   - Method and path: 'GET /pet/{petId}' (will generate tests only for that specific method)
   When focusing on a single endpoint, generate more comprehensive test coverage including edge cases.

5. Create the test project structure using filesystem tools:
   - **CRITICAL**: YOU must directly create all files using the filesystem tool
   - DO NOT try to execute "api_test_create" or any external command
   
   **CRITICAL - Check for existing project FIRST:**
   - **ALWAYS check for existing .csproj files before creating anything**
   - Search in the following locations (in order):
     1. Current working directory (where the command is run)
     2. The output_dir specified by the user
     3. Parent directory of output_dir
     4. Any subdirectories in the current working directory
   - Use filesystem tool to list files and look for *.csproj files
   - If ANY .csproj file is found: **USE THE EXISTING PROJECT** (do NOT create a new one)
   
   **For EXISTING projects (if .csproj found):**
   - **DO NOT create a new .csproj file**
   - **DO NOT create a new project structure**
   - Only create test class files (.cs files) in the output_dir
   - Assume RestAssured.Net and test framework packages are already installed
   - Use the namespace from the existing project OR use the namespace parameter if provided
   - Add tests to the existing project structure
   - Example: If project exists at `/MyProject/MyProject.csproj`, create test files in output_dir (e.g., `/MyProject/ApiTests/PetApiTests.cs`)
   
   **For NEW projects only (if NO .csproj found anywhere):**
   - Create the output directory if it doesn't exist (use filesystem tools)
   - Generate a .csproj file with required NuGet packages:
     * RestAssured.Net (latest version)
     * NHamcrest (for Body matchers like NHamcrest.Is.EqualTo)
     * NUnit/xUnit/MSTest (based on test_framework parameter)
     * NUnit.Analyzers or equivalent (if using NUnit)
     * Microsoft.NET.Test.Sdk
   - Create a test configuration file for base URL and common settings
   - Note: System.Text.Json is included by default in .NET 6.0+

6. Generate C# test classes:
   - **CRITICAL - Determine Namespace**:
     * Before generating test classes, check for existing test files in the output_dir
     * List all .cs files in the output_dir (e.g., `*Tests.cs`, `*Test.cs`)
     * If existing test files are found, read one of them and extract the namespace
     * Look for the `namespace` declaration (e.g., `namespace MyProject.Tests` or `namespace MyProject.Tests;`)
     * Use the SAME namespace for the new test class to maintain consistency
     * If NO existing test files are found, use the namespace parameter provided by the user
     * If namespace parameter is not provided and no existing tests found, derive from project name or use "ApiTests"
   - One test class per API resource/controller
   - Use RestAssured.Net fluent API (Given().When().Then() pattern)
   - Include proper test attributes ([Test], [Fact], [TestMethod])
   - Add descriptive test names following naming conventions
   - Implement setup/teardown methods for base URL configuration

7. For each endpoint, generate:
   **Positive Tests:**
   - Valid request with all required parameters
   - Verify response status code
   - **CRITICAL - Validate response body against schema**: For each property in the response schema:
     * Extract the property from the response using JSONPath (e.g., `$.id`, `$.name`, `$.status`)
     * Validate the property value matches the expected type and constraints
     * For POST/PUT requests: Validate that the response contains the same data that was sent in the request
     * For GET requests: Validate that all required fields from the schema are present and not empty
     * Example: If Pet schema has `id`, `name`, `status`, `photoUrls`, validate each field:
       - `.Body("$.id", NHamcrest.Is.GreaterThan(0))` for numeric IDs
       - `.Body("$.name", NHamcrest.Is.EqualTo(petName))` for exact match with request data
       - `.Body("$.status", NHamcrest.Is.EqualTo("available"))` for enum values
   - Check response headers if applicable

   **Negative Tests** (if include_negative_tests=true):
   - Missing required parameters
   - Invalid data types
   - Unauthorized access (401)
   - Forbidden access (403)
   - Invalid HTTP methods (405)
   - Malformed request body

8. Test Code Structure Example:
   ```csharp
   using NUnit.Framework;
   using static RestAssured.Dsl;
   using NHamcrest.Core;
   using System;
   
   namespace {namespace}
   {
       [TestFixture]
       public class {Resource}ApiTests
       {
           private string _baseUrl;
           private Random _random;
           
           [SetUp]
           public void Setup()
           {
               _baseUrl = "{base_url or from spec}";
               _random = new Random();
           }
           
           [Test]
           public void Get{Resource}_ValidRequest_ReturnsOk()
           {
               Given()
                   .ContentType("application/json")
                   .Accept("application/json")
               .When()
                   .Get($"{_baseUrl}/api/{endpoint}")
               .Then()
                   .StatusCode(200);
           }
           
           [Test]
           public void Post{Resource}_ValidData_ReturnsSuccess()
           {
               // Generate unique test data with random number
               // Use min/max from schema if available, otherwise use defaults
               var randomId = _random.Next(10000, 99999);
               var uniqueName = $"Test Resource {randomId}";
               
               var requestBody = new 
               {
                   name = uniqueName,
                   value = "Test Value"
               };
               
               Given()
                   .ContentType("application/json")
                   .Body(requestBody)
               .When()
                   .Post($"{_baseUrl}/api/{endpoint}")
               .Then()
                   .StatusCode(201) // Use actual status code from Swagger
                   .Body("$.name", NHamcrest.Is.EqualTo(uniqueName));
           }
           
           [Test]
           public void Post{Resource}_ValidData_ReturnsCreatedResourceWithMatchingFields()
           {
               // Generate unique test data using schema constraints
               // Example: If Port has minimum=1024, maximum=65535 in schema, use those values
               var randomNumber = _random.Next(1000, 9999); // Use schema min/max if available
               var newServer = new 
               {
                   Key = $"server-{randomNumber}",
                   Name = $"Test Server {randomNumber}",
                   Port = _random.Next(1024, 65535) // Use actual min/max from schema
               };
               
               Given()
                   .ContentType("application/json")
                   .Body(newServer)
               .When()
                   .Post($"{_baseUrl}/api/{endpoint}")
               .Then()
                   .StatusCode(201)
                   .Body("$.Key", NHamcrest.Is.EqualTo(newServer.Key))
                   .Body("$.Name", NHamcrest.Is.EqualTo(newServer.Name))
                   .Body("$.Port", NHamcrest.Is.EqualTo(newServer.Port));
           }
           
           [Test]
           public void Post{Resource}_ValidData_ValidatesAllResponseFields()
           {
               // Example for Pet API: Validate all fields in response match schema
               var randomId = _random.Next(1000, 9999);
               var petName = $"TestPet-{randomId}";
               var petStatus = "available"; // Use ONE valid enum value
               var newPet = new 
               {
                   id = randomId,
                   name = petName,
                   status = petStatus,
                   photoUrls = new[] { "http://example.com/photo.jpg" }
               };
               
               Given()
                   .ContentType("application/json")
                   .Body(newPet)
               .When()
                   .Post($"{_baseUrl}/api/pet")
               .Then()
                   .StatusCode(200) // Use actual status from Swagger
                   .Body("$.id", NHamcrest.Is.EqualTo(newPet.id))
                   .Body("$.name", NHamcrest.Is.EqualTo(newPet.name))
                   .Body("$.status", NHamcrest.Is.EqualTo(petStatus)) // Validate exact value sent
                   .Body("$.photoUrls[0]", NHamcrest.Is.EqualTo(newPet.photoUrls[0]));
           }
           
           [Test]
           public void Get{Resource}_InvalidId_ReturnsNotFound()
           {
               var invalidId = _random.Next(900000, 999999);
               
               Given()
               .When()
                   .Get($"{_baseUrl}/api/{endpoint}/{invalidId}")
               .Then()
                   .StatusCode(404);
           }
           
           [Test]
           public void Post{Resource}_MissingRequiredField_ReturnsBadRequest()
           {
               var invalidBody = new { };
               
               Given()
                   .ContentType("application/json")
                   .Body(invalidBody)
               .When()
                   .Post($"{_baseUrl}/api/{endpoint}")
               .Then()
                   .StatusCode(400);
           }
       }
   }
   ```

9. Create supporting files using filesystem tools:
   - appsettings.json for test configuration
   - Helper classes for common test utilities (if needed)

10. **CRITICAL - Compile and Auto-Fix**:
   After generating test files, ALWAYS attempt to compile and fix any errors:
   
   **Step 1: Restore packages**
   - If .csproj file exists in output_dir: Run `dotnet restore {output_dir}/*.csproj`
   - If .csproj file exists in parent directory: Run `dotnet restore {parent_dir}/*.csproj`
   - Use the full path to the .csproj file when running dotnet commands
   - If this fails, check for .csproj issues
   
   **Step 2: Build and detect errors**
   - Run: `dotnet build {path_to_csproj}` using the full path to the .csproj file
   - Capture the build output and look for compilation errors
   
   **Step 3: Auto-fix compilation errors**
   If build fails, analyze the error messages and fix common issues:
   
   a) **Missing using directives**:
      - Error: "The type or namespace name 'X' could not be found"
      - Fix: Add missing `using` statements at the top of the file
      - Example: Add `using System.Linq;` if LINQ methods are used
   
   b) **ExtractableResponse errors** (CS1061):
      - Error: "'ExtractableResponse' does not contain a definition for 'StatusCode'"
      - Fix: Remove `.Extract()` calls and use fluent API in `.Then()` block
      - Replace: `var response = Given().When().Get(...).Then().Extract(); response.StatusCode(200);`
      - With: `Given().When().Get(...).Then().StatusCode(200);`
   
   c) **Invalid method calls** (e.g., .If()):
      - Error: "'RequestSpecification' does not contain a definition for 'If'"
      - Fix: Remove `.If()` calls and use C# if statements or separate test methods
      - Replace: `.If(token != null, req => req.Header(...))`
      - With: Standard C# conditional logic before Given()
   
   d) **NHamcrest matcher errors**:
      - Error: "Cannot resolve method 'AnyOf'"
      - Fix: Use `NHamcrest.Is.EqualTo()` for single value validation
      - Replace: `NHamcrest.Is.AnyOf(...)`
      - With: `NHamcrest.Is.EqualTo(specificValue)`
   
   e) **Type mismatch errors**:
      - Error: "Operator '&&' cannot be applied to operands of type 'bool' and 'string'"
      - Fix: Correct the assertion logic to use proper types
   
   **Step 4: Rebuild after fixes**
   - After making fixes, run `dotnet build` again
   - Repeat the fix process up to 3 times if needed
   - If still failing after 3 attempts, report the remaining errors
   
   **Step 5: Optional test execution**
   - If build succeeds, optionally run `dotnet test` to verify tests execute
   - Note: Tests may fail due to API availability, but compilation must succeed
   
   **Example auto-fix workflow**:
   ```
   1. Generate test files
   2. Run: dotnet restore
   3. Run: dotnet build
   4. If errors found:
      - Parse error messages
      - Identify error type (missing using, .Extract(), .If(), etc.)
      - Read the problematic file
      - Fix the specific lines with errors
      - Write the corrected file back
      - Run: dotnet build again
   5. Repeat until build succeeds or max attempts reached
   6. Return success with any remaining warnings
   ```

11. Return structured output with:
   - success: true if generation AND compilation completed successfully
   - test_files_created: array of created file paths (ONLY files actually created, not existing files)
   - test_count: number of test methods generated
   - endpoints_covered: number of endpoints tested
   - dependencies: list of required NuGet packages
   - existing_project: true/false indicating if an existing project was detected and used
   - project_file: path to the .csproj file (existing or newly created)
   - **compilation_status**: "success" | "failed" | "fixed" (indicates if auto-fix was needed)
   - **build_attempts**: number of build attempts made
   - **errors_fixed**: array of error types that were automatically fixed
   - **remaining_errors**: array of any compilation errors that could not be auto-fixed

## Filesystem Operations:
You MUST use the filesystem tools to:
- **FIRST**: List files in current directory and output_dir to check for existing .csproj files
- Create directories only if needed (e.g., `{output_dir}/` if it doesn't exist)
- Write test class files (.cs files)
- Write .csproj and supporting files ONLY if no existing project is found

**Example workflow for EXISTING project:**
1. List files in current directory: Check for *.csproj
2. If .csproj found: Note the project location
3. Create output_dir if it doesn't exist: `{output_dir}/`
4. Write ONLY test class files: `{output_dir}/{Resource}ApiTests.cs`
5. Do NOT create .csproj or project files

**Example workflow for NEW project:**
1. List files in current directory: Check for *.csproj
2. If NO .csproj found: Proceed with new project creation
3. Create output directory: `{output_dir}/`
4. Write .csproj file: `{output_dir}/{ProjectName}.csproj`
5. Write test class files: `{output_dir}/{Resource}ApiTests.cs`
6. Write supporting files: `{output_dir}/README.md`, `{output_dir}/appsettings.json`

## RestAssured.Net API Testing Syntax:
**IMPORTANT**: Use RestAssured.Net fluent API for clean, BDD-style C# API tests. Key patterns:

- Import: `using static RestAssured.Dsl;` and `using NHamcrest.Core;` (for matchers)
- Fluent pattern: `Given().When().Then()` (after using static import)
- Alternative: `RestAssured.Dsl.Given().When().Then()` (without static import)
- Full URL: Use complete URL in HTTP method calls: `.Get($"{baseUrl}/api/endpoint")`
- Content-Type: `.ContentType("application/json")` (NOT .Header("Content-Type", ...))
- Accept: `.Accept("application/json")` (NOT .Header("Accept", ...))
- Custom headers: `.Header("X-Custom-Header", "value")`
- **CRITICAL**: RestAssured.Net does NOT support `.If()` for conditional logic
- For conditional headers, use standard C# if statements BEFORE the Given() call, or create separate test methods
- Body: `.Body(objectOrJson)`
- Path parameters: `.PathParam("id", value)`
- Query parameters: `.QueryParam("page", "1")`
- Status code: `.StatusCode(200)` or `.StatusCode(HttpStatusCode.OK)`
- Body assertions: `.Body("$.property", NHamcrest.Is.EqualTo("value"))` (uses NHamcrest matchers)
- Enum validation: For enum fields, validate against ONE specific valid value, NOT multiple values
  - ✅ Correct: `.Body("$.status", NHamcrest.Is.EqualTo("available"))`
  - ❌ Wrong: `.Body("$.status", NHamcrest.Is.AnyOf(...))` (AnyOf is not a valid NHamcrest matcher)

**CRITICAL**: Always use the correct import and fluent chain pattern!
- ✅ Correct: `using static RestAssured.Dsl;` then use `Given()`
- ✅ Alternative: `using RestAssured.Dsl;` then use `RestAssured.Dsl.Given()`
- ✅ Correct: Use the complete fluent chain: `Given()...When()...Then()...`
- ✅ Correct: Body assertions use NHamcrest matchers with comma separator: `.Body("$.field", NHamcrest.Is.EqualTo(value))`
- ✅ Correct: Use `NHamcrest.Is.EqualTo(value)` for exact matches
- ✅ Correct: For enum fields, use the SAME value that was sent in the request: `.Body("$.status", NHamcrest.Is.EqualTo(requestData.status))`

**Common RestAssured.Net Patterns:**

```csharp
using static RestAssured.Dsl;
using NHamcrest.Core;

// GET request with validation
Given()
    .Accept("application/json")
    .PathParam("id", userId)
.When()
    .Get($"{baseUrl}/api/users/{id}")
.Then()
    .StatusCode(200)
    .Body("$.id", NHamcrest.Is.EqualTo(userId))


// POST request with body - with random number for uniqueness
var random = new Random();
var randomId = random.Next(10000, 99999);
var newUser = new { 
    Name = $"John-{randomId}", 
    Email = $"john{randomId}@example.com" 
};

Given()
    .ContentType("application/json")
    .Body(newUser)
.When()
    .Post($"{baseUrl}/api/users")
.Then()
    .StatusCode(201)
    .Body("$.Name", NHamcrest.Is.EqualTo(newUser.Name))
    .Body("$.Email", NHamcrest.Is.EqualTo(newUser.Email));

// POST with GUID for unique identifiers
var uniqueKey = Guid.NewGuid().ToString();
var newServer = new { 
    Key = uniqueKey, 
    Name = $"Server-{uniqueKey.Substring(0, 8)}" 
};

Given()
    .ContentType("application/json")
    .Body(newServer)
.When()
    .Post($"{baseUrl}/api/servers")
.Then()
    .StatusCode(201)
    .Body("$.Key", NHamcrest.Is.EqualTo(newServer.Key))
    .Body("$.Name", NHamcrest.Is.EqualTo(newServer.Name));

// PUT request with authentication
var updatedUser = new { Name = "John Updated", Email = "john.updated@example.com" };

Given()
    .Header("Authorization", $"Bearer {token}")
    .ContentType("application/json")
    .PathParam("id", userId)
    .Body(updatedUser)
.When()
    .Put($"{baseUrl}/api/users/{id}")
.Then()
    .StatusCode(200);

// WRONG: Do NOT use .If() - it doesn't exist in RestAssured.Net
// var request = Given();
// request.If(token != null, req => req.Header("Authorization", $"Bearer {token}")); // WRONG!

// CORRECT: Add headers directly or use C# if statements
var request = Given()
    .ContentType("application/json");

if (!string.IsNullOrEmpty(token))
{
    request = request.Header("Authorization", $"Bearer {token}");
}

request
.When()
    .Get($"{baseUrl}/api/users")
.Then()
    .StatusCode(200);

// DELETE request
Given()
    .PathParam("id", userId)
.When()
    .Delete($"{baseUrl}/api/users/{id}")
.Then()
    .StatusCode(204);

// Query parameters
Given()
    .QueryParam("page", "1")
    .QueryParam("limit", "10")
.When()
    .Get($"{baseUrl}/api/users")
.Then()
    .StatusCode(200)

// DO NOT use .Extract() - it returns ExtractableResponse which doesn't support fluent assertions
// Always use direct assertions in .Then() block
Given()
.When()
    .Get("/api/users/1")
.Then()
    .StatusCode(200)
    .Body("$.id", NHamcrest.Is.EqualTo(1));
```

**CRITICAL - DO NOT USE .Extract():**
- `.Extract()` returns an `ExtractableResponse` object that does NOT support fluent assertions
- DO NOT use patterns like: `.Extract().StatusCode()` or `.Extract().ContentType()`
- ALWAYS use direct assertions in the `.Then()` block
- Example of WRONG code:
  ```csharp
  var response = Given().When().Get("/api/users").Then().Extract();
  Assert.That(response.StatusCode(), Is.EqualTo(200)); // WRONG - will not compile
  ```
- Example of CORRECT code:
  ```csharp
  Given()
  .When()
      .Get("/api/users")
  .Then()
      .StatusCode(200)
      .Body("$.length()", NHamcrest.Is.GreaterThan(0));
  ```

## Test Data Generation with Random Values:
**CRITICAL**: Always generate unique test data to avoid conflicts and ensure test isolation.
**CRITICAL**: Always read min/max constraints from the Swagger/OpenAPI schema and use them when generating random values.

1. **Random Numbers with Schema Constraints**: Use `Random` class with min/max from schema
   ```csharp
   private Random _random = new Random();
   
   // Read minimum/maximum from schema property definition
   // Example: If schema defines "age": {"type": "integer", "minimum": 18, "maximum": 120}
   var age = _random.Next(18, 121); // maximum is exclusive in Random.Next()
   
   // Example: If schema defines "port": {"type": "integer", "minimum": 1024, "maximum": 65535}
   var port = _random.Next(1024, 65536);
   
   // If no constraints in schema, use sensible defaults
   var randomId = _random.Next(10000, 99999);
   ```

   **How to extract constraints from Swagger schema:**
   - Look for `minimum` and `maximum` properties in the schema definition
   - For integers: Use exact values from schema (remember Random.Next max is exclusive)
   - For numbers (float/double): Use schema values or cast appropriately
   - If constraints are missing, use reasonable defaults based on the field name/type

2. **Strings with Length Constraints**: Use minLength/maxLength from schema
   ```csharp
   // Example: If schema defines "username": {"type": "string", "minLength": 3, "maxLength": 20}
   var randomSuffix = _random.Next(1000, 9999);
   var username = $"user{randomSuffix}"; // Ensure length is between 3 and 20
   
   // Example: If schema defines "description": {"type": "string", "maxLength": 255}
   var description = $"Test description {randomSuffix}".Substring(0, Math.Min(255, $"Test description {randomSuffix}".Length));
   ```

3. **Enums**: Use ONE value from enum definition in schema
   ```csharp
   // Example: If schema defines "status": {"type": "string", "enum": ["active", "inactive", "pending"]}
   // Pick ONE valid value for the test (preferably the first one or a sensible default)
   var status = "active"; // Use a single valid enum value
   
   // Alternative: Randomly select one value if you want variety across tests
   var validStatuses = new[] { "active", "inactive", "pending" };
   var status = validStatuses[_random.Next(validStatuses.Length)];
   
   // CRITICAL: When validating response, check for the EXACT value sent in request
   // ✅ Correct: .Body("$.status", NHamcrest.Is.EqualTo(status))
   // ❌ Wrong: .Body("$.status", NHamcrest.Is.AnyOf(...)) - AnyOf is not valid
   ```

4. **GUIDs**: Use `Guid.NewGuid()` for globally unique identifiers (when format is "uuid")
   ```csharp
   // Example: If schema defines "id": {"type": "string", "format": "uuid"}
   var uniqueKey = Guid.NewGuid().ToString();
   var shortId = Guid.NewGuid().ToString().Substring(0, 8);
   ```

5. **Timestamps**: Use current timestamp for date/datetime fields
   ```csharp
   // Example: If schema defines "createdAt": {"type": "string", "format": "date-time"}
   var timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds();
   var dateTime = DateTime.UtcNow.ToString("o"); // ISO 8601 format
   ```

6. **Combined Approach**: Mix random numbers with descriptive names, respecting constraints
   ```csharp
   var randomSuffix = _random.Next(1, 9999);
   var testData = new {
       Key = $"test-key-{randomSuffix}",
       Name = $"Test Resource {randomSuffix}",
       Email = $"test{randomSuffix}@example.com",
       Age = _random.Next(18, 121), // From schema: minimum=18, maximum=120
       Port = _random.Next(1024, 65536) // From schema: minimum=1024, maximum=65535
   };
   ```

**Best Practice**: Initialize `Random` in the `[SetUp]` method and reuse it across tests:
```csharp
private Random _random;

[SetUp]
public void Setup()
{
    _baseUrl = "https://api.example.com";
    _random = new Random();
}
```

**Schema Constraint Extraction Process:**
1. Parse the Swagger/OpenAPI specification JSON
2. For each endpoint's request body schema, extract the `properties` object
3. For each property, check for validation constraints:
   - `type`: Determines the data type (integer, number, string, boolean, array, object)
   - `minimum`, `maximum`: For numeric types
   - `minLength`, `maxLength`: For string types
   - `minItems`, `maxItems`: For array types
   - `enum`: For enumerated values
   - `format`: For specific formats (date, date-time, email, uuid, etc.)
   - `pattern`: For regex validation (use to generate valid strings)
4. Use these constraints when generating test data to ensure values are valid
5. If constraints are missing, use sensible defaults based on field name and type

**Example Schema Analysis:**
```json
{
  "Server": {
    "type": "object",
    "properties": {
      "Key": {"type": "string", "minLength": 5, "maxLength": 50},
      "Name": {"type": "string", "maxLength": 100},
      "Port": {"type": "integer", "minimum": 1024, "maximum": 65535},
      "Status": {"type": "string", "enum": ["active", "inactive"]},
      "CreatedAt": {"type": "string", "format": "date-time"}
    }
  }
}
```

**Generated Test Code:**
```csharp
var randomNumber = _random.Next(1000, 9999);
var newServer = new {
    Key = $"srv-{randomNumber}", // Length: 9 (within 5-50)
    Name = $"Test Server {randomNumber}", // Length check against maxLength 100
    Port = _random.Next(1024, 65536), // Within minimum=1024, maximum=65535
    Status = "active", // From enum: ["active", "inactive"]
    CreatedAt = DateTime.UtcNow.ToString("o") // ISO 8601 format for date-time
};
```

## Best Practices:
- Use meaningful test method names (e.g., GetUser_WithValidId_ReturnsUser)
- Add XML documentation comments to test classes
- Use RestAssured.Net's fluent Given().When().Then() pattern for BDD-style tests
- Store base URL in SetUp method for reusability
- **ALWAYS generate unique test data using random numbers, GUIDs, or timestamps**
- Initialize `Random` instance in `[SetUp]` method for reuse across tests
- Use test data builders or fixtures for complex request bodies
- Handle authentication tokens properly (use .Header("Authorization", $"Bearer {token}"))
- Add comments explaining complex test scenarios
- Follow C# naming conventions (PascalCase for methods/classes)
- Organize tests by HTTP method and resource
- Use .PathParam() for path parameters instead of string interpolation
- Use .QueryParam() for query strings
- **Use .Body("$.jsonPath", NHamcrest.Is.EqualTo(expectedValue)) for exact JSON field validation**
- **CRITICAL**: Always use the full `NHamcrest.Is.EqualTo()` syntax in Body assertions, NOT just `Is.EqualTo()`
- Body assertions require the comma separator: `.Body("$.field", NHamcrest.Is.EqualTo(value))`
- **CRITICAL - Validate ALL response fields**: For each endpoint, validate ALL properties defined in the response schema
  * Read the response schema from Swagger (under "responses" -> "200" -> "schema")
  * For each property in the schema, add a `.Body()` assertion
  * Validate exact values for data sent in request: `.Body("$.name", NHamcrest.Is.EqualTo(requestData.name))`
  * Validate numeric fields are in valid range: `.Body("$.id", NHamcrest.Is.GreaterThan(0))`
  * **CRITICAL - Enum field validation**: For enum fields, validate against the EXACT value sent in the request
    - ✅ Correct: `.Body("$.status", NHamcrest.Is.EqualTo(requestData.status))`
    - ❌ Wrong: `.Body("$.status", NHamcrest.Is.AnyOf(...))` - AnyOf is not a valid NHamcrest matcher
    - Use ONE specific enum value in your test data and validate that exact value in the response
- Chain multiple assertions in .Then() block to validate multiple response fields
- Validate that POST/PUT responses return the same data that was sent (e.g., `.Body("$.Key", NHamcrest.Is.EqualTo(newServer.Key))`)
- Add test categories/traits for test organization ([Category("Smoke")], [Category("Integration")])
- **NEVER use .Extract()** - it breaks the fluent API and returns ExtractableResponse without assertion methods
- Always perform ALL validations in the .Then() block using the fluent API
- Keep tests readable and maintainable with clear Given/When/Then structure

## Error Handling:
**CRITICAL**: Always validate inputs and provide clear error messages:

1. **Swagger URL Validation**:
   - Check if the URL is accessible 

2. **API Endpoint Validation**:
   - If single_api parameter is provided, verify the endpoint exists in the swager documentation
   - If not found: "API endpoint '{single_api}' not found in Swagger. Available endpoints: {list_of_endpoints}"
   - Check if the HTTP method exists for the path
   - If method not found: "HTTP method not found for endpoint '{single_api}'. Available methods: {list_of_methods}"

3. **Response Code Validation**:
   - Verify each endpoint has a responses section
   - If missing: "Warning: Endpoint '{endpoint}' has no responses defined. Using default status codes."
   - Check for at least one success response code (2xx)
   - If none: "Warning: No success response codes defined for '{endpoint}'. Cannot generate positive tests."

4. **Schema Validation**:
   - Check if request body schema is defined when required
   - If missing: "Warning: Request body schema not defined for '{endpoint}'. Using generic object."
   - Validate response schema exists for success responses
   - If missing: "Warning: Response schema not defined for '{endpoint}'. Skipping response body validation."

5. **Base URL Validation**:
   - Extract base URL from Swagger spec (host, basePath, schemes)
   - If missing and base_url parameter not provided: "Error: Cannot determine base URL. Swagger spec missing 'host' field and no base_url parameter provided."
   - Validate base_url parameter format if provided
   - If invalid: "Invalid base_url format: '{base_url}'. Expected format: http(s)://domain:port"

6. **File System Errors**:
   - If output directory cannot be created: "Error: Failed to create output directory '{output_dir}'. Permission denied or invalid path."
   - If file write fails: "Error: Failed to write file '{file_path}'. {error_message}"

7. **Summary Report**:
   - Always provide a summary even if errors occur
   - Include: what succeeded, what failed, and actionable next steps
   - Example: "Successfully analyzed 5 endpoints, failed to parse 2 endpoints due to missing schemas. Generated 15 test methods in 3 files."

**Return Format on Error**:
```json
{
  "success": false,
  "error_message": "Clear description of what went wrong",
  "error_type": "URL_NOT_ACCESSIBLE|INVALID_JSON|ENDPOINT_NOT_FOUND|etc",
  "suggestion": "Actionable suggestion to fix the issue"
}
```
"""
